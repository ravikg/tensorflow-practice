{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function to sentence(word seq.) to char seq(space is treated as special characted)\n",
    "SPACE = \"_SPACE_\"\n",
    "def char_seq(sentence) :\n",
    "    char_seq_output = \"\"\n",
    "    for c in sentence:\n",
    "        char_seq_output += \" \"\n",
    "        if c == \" \":\n",
    "            char_seq_output += SPACE\n",
    "        else:\n",
    "            char_seq_output += c\n",
    "    return char_seq_output.lstrip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d _SPACE_ u v w x y z', '1 2 3 _SPACE_ 9 8 7']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(char_seq, [\"abcd uvwxyz\", \"123 987\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of lines 2791\n",
      "{'_space_': 1, 'e': 2, 't': 3, 'a': 4, 'o': 5, 'i': 6, 'n': 7, 'h': 8, 's': 9, 'r': 10, 'd': 11, 'l': 12, 'u': 13, 'c': 14, 'w': 15, 'g': 16, 'y': 17, ',': 18, 'm': 19, 'f': 20, 'p': 21, '’': 22, 'b': 23, 'k': 24, '.': 25, '‘': 26, 'v': 27, '-': 28, '!': 29, ':': 30, 'j': 31, 'q': 32, '?': 33, ';': 34, 'x': 35, '*': 36, 'z': 37, ')': 38, '“': 39, '(': 40, '1': 41, '”': 42, '/': 43, '0': 44, '5': 45, '3': 46, '2': 47, '8': 48, '9': 49, '4': 50, '6': 51, '[': 52, '7': 53, '_': 54, ']': 55, '@': 56, '$': 57, '#': 58, '%': 59}\n",
      "sample seq:\n",
      "  line 1:\n",
      "    text  > project gutenberg’s alice’s adventures in wonderland , by lewis carroll\n",
      "    seq   > [21, 10, 5, 31, 2, 14, 3, 1, 16, 13, 3, 2, 7, 23, 2, 10, 16, 22, 9, 1, 4, 12, 6, 14, 2, 22, 9, 1, 4, 11, 27, 2, 7, 3, 13, 10, 2, 9, 1, 6, 7, 1, 15, 5, 7, 11, 2, 10, 12, 4, 7, 11, 1, 18, 1, 23, 17, 1, 12, 2, 15, 6, 9, 1, 14, 4, 10, 10, 5, 12, 12]\n",
      "  line 2:\n",
      "    text  > this ebook is for the use of anyone anywhere at no cost and with\n",
      "    seq   > [3, 8, 6, 9, 1, 2, 23, 5, 5, 24, 1, 6, 9, 1, 20, 5, 10, 1, 3, 8, 2, 1, 13, 9, 2, 1, 5, 20, 1, 4, 7, 17, 5, 7, 2, 1, 4, 7, 17, 15, 8, 2, 10, 2, 1, 4, 3, 1, 7, 5, 1, 14, 5, 9, 3, 1, 4, 7, 11, 1, 15, 6, 3, 8]\n"
     ]
    }
   ],
   "source": [
    "# pre process the input data : alice.en\n",
    "with open('./data/alice.en') as f:\n",
    "    input_lines = f.read().splitlines()\n",
    "    input_lines_cs = list(map(char_seq, input_lines))\n",
    "print('no. of lines {}'.format(len(input_lines)))\n",
    "# tokenizer which makes the text lower case and split by space\n",
    "en_tokenizer = Tokenizer(lower=True, split=\" \", filters='')\n",
    "en_tokenizer.fit_on_texts(input_lines_cs)\n",
    "\n",
    "# vocabulary:\n",
    "print(en_tokenizer.word_index)\n",
    "\n",
    "# get sequences\n",
    "input_seqs = en_tokenizer.texts_to_sequences(input_lines_cs)\n",
    "print('sample seq:')\n",
    "for i, (line, seq) in enumerate(zip(input_lines[:2], input_seqs[:2])):\n",
    "    print('  line {}:'.format(i + 1))\n",
    "    print('    text  > {}'.format(line))\n",
    "    print('    seq   > {}'.format(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_tokenizer.word_index)\n",
    "type(input_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy\\nprint(input_seqs[:10])\\ninput_seqs_np = numpy.array([numpy.array(xi) for xi in input_seqs])\\nprint(input_seqs_np)\\ntype(input_seqs_np)\\n#input_seqs_tensor = tf.convert_to_tensor(input_seqs_np)\\n#print(input_seqs_tensor)\\nbatched_data = tf.train.batch(\\n    tensors=[input_seqs_np],\\n    batch_size=10,\\n    dynamic_pad=True,\\n    name=\"input_seq_batch\"\\n)\\n# Run the graph\\n# tf.contrib.learn takes care of starting the queues for us\\nres = tf.contrib.learn.run_n({\"y\": batched_data}, n=1, feed_dict=None)\\n# Print the result\\n#print(\"Batch shape: {}\".format(res[0][\"y\"].shape))\\nprint(res[0][\"y\"])\\nprint(res[0])\\nres.shape\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying use batch method from tensorflow\n",
    "'''\n",
    "import numpy\n",
    "print(input_seqs[:10])\n",
    "input_seqs_np = numpy.array([numpy.array(xi) for xi in input_seqs])\n",
    "print(input_seqs_np)\n",
    "type(input_seqs_np)\n",
    "#input_seqs_tensor = tf.convert_to_tensor(input_seqs_np)\n",
    "#print(input_seqs_tensor)\n",
    "batched_data = tf.train.batch(\n",
    "    tensors=[input_seqs_np],\n",
    "    batch_size=10,\n",
    "    dynamic_pad=True,\n",
    "    name=\"input_seq_batch\"\n",
    ")\n",
    "# Run the graph\n",
    "# tf.contrib.learn takes care of starting the queues for us\n",
    "res = tf.contrib.learn.run_n({\"y\": batched_data}, n=1, feed_dict=None)\n",
    "# Print the result\n",
    "#print(\"Batch shape: {}\".format(res[0][\"y\"].shape))\n",
    "print(res[0][\"y\"])\n",
    "print(res[0])\n",
    "res.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of lines 2791\n",
      "vocabulary size: 59\n",
      "vocabulary: {'_space_': 1, '5': 2, 'x': 3, '4': 4, 'w': 5, 'f': 6, '?': 7, ']': 8, 'u': 9, 'c': 10, 't': 11, '-': 12, '6': 13, '7': 14, 'r': 15, '’': 16, '[': 17, '$': 18, ':': 19, ';': 20, 's': 21, '”': 22, 'e': 23, '!': 24, 'q': 25, '_': 26, '/': 27, '1': 28, '#': 29, 'p': 30, '%': 31, 'v': 32, 'k': 33, '9': 34, '“': 35, 'b': 36, '‘': 37, 'd': 38, '8': 39, 'z': 40, 'y': 41, ')': 42, '2': 43, '(': 44, '3': 45, 'l': 46, 'h': 47, 'g': 48, 'o': 49, 'j': 50, ',': 51, 'm': 52, '0': 53, '.': 54, '@': 55, '*': 56, 'n': 57, 'i': 58, 'a': 59}\n",
      "sample seq:\n",
      "  line 1:\n",
      "    text  > scw%57x ’6x5?e5c’”u 4-f75”u 4t/5?x6c5u f? rw?t5c-4?t $ e[ -5rfu 74ccw--\n",
      "    seq   > [21, 10, 5, 31, 2, 14, 3, 1, 16, 13, 3, 2, 7, 23, 2, 10, 16, 22, 9, 1, 4, 12, 6, 14, 2, 22, 9, 1, 4, 11, 27, 2, 7, 3, 13, 10, 2, 9, 1, 6, 7, 1, 15, 5, 7, 11, 2, 10, 12, 4, 7, 11, 1, 18, 1, 23, 17, 1, 12, 2, 15, 6, 9, 1, 14, 4, 10, 10, 5, 12, 12]\n",
      "  line 2:\n",
      "    text  > x]fu 5eww! fu ;wc x]5 6u5 w; 4?[w?5 4?[r]5c5 4x ?w 7wux 4?t rfx]\n",
      "    seq   > [3, 8, 6, 9, 1, 2, 23, 5, 5, 24, 1, 6, 9, 1, 20, 5, 10, 1, 3, 8, 2, 1, 13, 9, 2, 1, 5, 20, 1, 4, 7, 17, 5, 7, 2, 1, 4, 7, 17, 15, 8, 2, 10, 2, 1, 4, 3, 1, 7, 5, 1, 14, 5, 9, 3, 1, 4, 7, 11, 1, 15, 6, 3, 8]\n"
     ]
    }
   ],
   "source": [
    "# pre process the output data : alice.x\n",
    "with open('./data/alice.x') as f:\n",
    "    output_lines = f.read().splitlines()\n",
    "    output_lines_cs = list(map(char_seq, output_lines))\n",
    "print('no. of lines {}'.format(len(output_lines)))\n",
    "# tokenizer which makes the text lower case and split by space\n",
    "x_tokenizer = Tokenizer(lower=True, split=\" \", filters='')\n",
    "x_tokenizer.fit_on_texts(output_lines_cs)\n",
    "\n",
    "# vocabulary:\n",
    "print('vocabulary size: {}'.format(len(x_tokenizer.word_index)))\n",
    "print('vocabulary: {}'.format(x_tokenizer.word_index))\n",
    "\n",
    "# get sequences\n",
    "output_seqs = x_tokenizer.texts_to_sequences(output_lines_cs)\n",
    "print('sample seq:')\n",
    "for i, (line, seq) in enumerate(zip(output_lines[:2], output_seqs[:2])):\n",
    "    print('  line {}:'.format(i + 1))\n",
    "    print('    text  > {}'.format(line))\n",
    "    print('    seq   > {}'.format(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do batching\n",
    "import helpers\n",
    "xt, xlen = helpers.batch(input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "(84, 2791)\n"
     ]
    }
   ],
   "source": [
    "print(len(xt))\n",
    "print(xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21  3  4 ...,  6  4  9]]\n"
     ]
    }
   ],
   "source": [
    "print(xt[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21  3  4 ...,  6  4  9]\n",
      " [10  8 12 ...,  7 10 13]\n",
      " [ 5  6 19 ..., 14 14 23]\n",
      " ..., \n",
      " [ 1  5  7 ...,  7  1 23]\n",
      " [16  5  5 ..., 16 20  2]\n",
      " [13 24  1 ...,  1  5  1]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(xt[:1])\n",
    "print(type(xt))\n",
    "print(type(xt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1 # this is not being used\n",
    "\n",
    "vocab_size = len(en_tokenizer.word_index)+1\n",
    "input_embedding_size = 16 # 20 or 32\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholders for encoder inputs, decoder input and decoder targets\n",
    "# we don't define the shape (size) of encoder inputs / decoder targets as it will depend on batch size\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "# similarly decoder inputs' shape (size) is dynamic - batch dependent\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')\n",
    "# ^^^ gets map to previous decoder output during rollout - but during training we want to \n",
    "#   input the target inspite of whatever the decoder output is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random initialization of embedding\n",
    "# word embedding help us reduce the dimension of data for network training\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to understand how this embedding look up works ??\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" now we don't delete encoder_output as it will be use in attention for decoder\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define encoder cell - using an LSTMCell\n",
    "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "# defined attention length randomnly - need to analysis the input data and maybe define based on that\n",
    "attention_length = 10 \n",
    "# add a attention wrapper around encoder cell\n",
    "encoder_cell_w_attention = tf.contrib.rnn.AttentionCellWrapper(encoder_cell, attention_length, state_is_tuple=True)\n",
    "# define encoder rnn\n",
    "encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell_w_attention, encoder_inputs_embedded,\n",
    "    dtype=tf.float32, time_major=True,\n",
    ")\n",
    "\n",
    "\n",
    "''' now we don't delete encoder_output as it will be use in attention for decoder'''\n",
    "#del encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 20) dtype=float32>),\n",
       " <tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 20) dtype=float32>,\n",
       " <tf.Tensor 'rnn/while/Exit_5:0' shape=(?, 200) dtype=float32>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_state # context vector or thought vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define decoder cell - using an LSTMCell\n",
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "decoder_cell_attn = tf.contrib.rnn.AttentionCellWrapper(decoder_cell, attention_length, state_is_tuple=True)\n",
    "# define decoder RNN\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "    decoder_cell_attn, decoder_inputs_embedded,\n",
    "    # using only the 1st value of attention based encoder rnn - some issue with size mismatch\n",
    "    initial_state=encoder_final_state,\n",
    "    dtype=tf.float32, time_major=True, scope=\"plain_decoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size)\n",
    "\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#decoder_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_ = input_seqs[:10]#[[6], [3, 4], [9, 8, 7]]\n",
    "dbatch_ = output_seqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbatch_, batch_length_ = helpers.batch(batch_)\\nprint('batch_encoded:\\n' + str(batch_))\\n\\n\\ndin_, dlen_ = helpers.batch(dbatch_)\\nprint('decoder inputs:\\n' + str(din_))\\n\\npred_ = sess.run(decoder_prediction,\\n    feed_dict={\\n        encoder_inputs: batch_,\\n        decoder_inputs: din_,\\n    })\\nprint('decoder predictions:\\n' + str(pred_))\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "batch_, batch_length_ = helpers.batch(batch_)\n",
    "print('batch_encoded:\\n' + str(batch_))\n",
    "\n",
    "\n",
    "din_, dlen_ = helpers.batch(dbatch_)\n",
    "print('decoder inputs:\\n' + str(din_))\n",
    "\n",
    "pred_ = sess.run(decoder_prediction,\n",
    "    feed_dict={\n",
    "        encoder_inputs: batch_,\n",
    "        decoder_inputs: din_,\n",
    "    })\n",
    "print('decoder predictions:\\n' + str(pred_))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get batch data\n",
    "def next_feed(batch_seq, input_seqs, output_seqs, batch_size):\n",
    "    start = batch_seq * batch_size\n",
    "    end = (batch_seq + 1) * batch_size\n",
    "    batch_ = input_seqs[start:end]\n",
    "    #print(batch_)\n",
    "    dbatch_ = output_seqs[start:end]\n",
    "    #print(batch_)\n",
    "    encoder_inputs_, _ = helpers.batch(batch_)\n",
    "    decoder_targets_, _ = helpers.batch(dbatch_)\n",
    "    decoder_inputs_, _ = helpers.batch(dbatch_)\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        decoder_inputs: decoder_inputs_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 200)\n"
     ]
    }
   ],
   "source": [
    "loss_track = []\n",
    "batch_size = 10\n",
    "max_batches = 200\n",
    "batches_in_epoch = 199\n",
    "training_data_size = max_batches * batch_size # 2000\n",
    "\n",
    "## get the validation data\n",
    "vd_batch_size = 200\n",
    "vd = next_feed(0, input_seqs[training_data_size:], output_seqs[training_data_size:], vd_batch_size)\n",
    "print(vd[encoder_inputs].shape)\n",
    "vd_loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> epoch 1:\n",
      "batch 0\n",
      "  minibatch loss: 4.1157917976379395\n",
      "  sample 1:\n",
      "    input     > [21 10  5 31  2 14  3  1 16 13  3  2  7 23  2 10 16 22  9  1  4 12  6 14  2\n",
      " 22  9  1  4 11 27  2  7  3 13 10  2  9  1  6  7  1 15  5  7 11  2 10 12  4\n",
      "  7 11  1 18  1 23 17  1 12  2 15  6  9  1 14  4 10 10  5 12 12]\n",
      "    predicted > [36 36 20 20 20 20 22 36 36 36 36 36 55 36 36 36 36 36 55 55 29  3  3 55 55\n",
      " 55 55 55 46 46 29 52 52 46 36 22 36 55 55 55 55 55 46  1 46 46 29 22 54 48\n",
      " 48 48 48 14 14  8 19 14 14  8 14 32 32  6 54 32 22 22 20 20 54]\n",
      "  sample 2:\n",
      "    input     > [ 3  8  6  9  1  2 23  5  5 24  1  6  9  1 20  5 10  1  3  8  2  1 13  9  2\n",
      "  1  5 20  1  4  7 17  5  7  2  1  4  7 17 15  8  2 10  2  1  4  3  1  7  5\n",
      "  1 14  5  9  3  1  4  7 11  1 15  6  3  8  0  0  0  0  0  0  0]\n",
      "    predicted > [20 20 33 33 20 35 33 33 39 39 39 55 55 55 55 55 22 55 55 36 36 55 36 55 55\n",
      " 55 55 55 55 29 55 29 29 29 29 54 29 22 29 29 29 29 22 22 54  7 22  8  7 20\n",
      " 20 20 20 54 54 54 28  8 48  8 14  8  8 22  8 14 14 14 14 14 14]\n",
      "  sample 3:\n",
      "    input     > [ 4 12 19  5  9  3  1  7  5  1 10  2  9  3 10  6 14  3  6  5  7  9  1 15  8\n",
      "  4  3  9  5  2 27  2 10  1 25  1 17  5 13  1 19  4 17  1 14  5 21 17  1  6\n",
      "  3  1 18  1 16  6 27  2  1  6  3  1  4 15  4 17  1  5 10  0  0]\n",
      "    predicted > [19 48 39 39 39 39 39 17 17 20 17 20 20 20 20 22 22 22 22 20 28  5  8 41  5\n",
      " 38  5  5  5  5 33  5 20 20  6  6  1  1  1  1 20 20  1  1 20 20 20 15  1 54\n",
      " 54 54 47 14 14 41 36 47 47 54 54 47 14 14 14 19 14 14 20 20 14]\n",
      "\n",
      "batch 199\n",
      "  minibatch loss: 1.4826985597610474\n",
      "  sample 1:\n",
      "    input     > [26  4  7 17  1  9  8 10  6 19 21  1 14  5 13 12 11  1  8  4 27  2  1  3  5\n",
      " 12 11  1 17  5 13  1  3  8  4  3  1 25  1 22  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "    predicted > [3 4 2 5 1 4 5 3 2 2 1 1 1 5 3 3 6 1 5 4 5 2 1 3 5 2 6 1 5 5 3 1 3 5 4 3 1\n",
      " 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n",
      "  sample 2:\n",
      "    input     > [26  6 20  1  6 22 11  1 23  2  2  7  1  3  8  2  1 15  8  6  3  6  7 16  1\n",
      " 18  1 22  1  9  4  6 11  1  4 12  6 14  2  1 18  1 15  8  5  9  2  1  3  8\n",
      "  5 13 16  8  3  9  1 15  2 10  2  1  9  3  6 12 12  1 10 13  7  7  6  7 16]\n",
      "    predicted > [ 3  2  2  1  2  2 11  1  4  2  2  2  1  3  8  2  1  1  5  2  3  2  2 11  1\n",
      " 18  1  2  1  3  5  2 11  1  4  3  2  1  2  1 18  1  1  8  5  3  2  1  3  8\n",
      "  5  3  6  5  3  3  1  1  2  3  2  1  3  3  6  3  3  1  3  4  2  2  6  2 11]\n",
      "  sample 3:\n",
      "    input     > [ 5  7  1  3  8  2  1  9  5  7 16  1 18  1 26  6 22 11  1  8  4 27  2  1  9\n",
      "  4  6 11  1  3  5  1  3  8  2  1 21  5 10 21  5  6  9  2  1 18  1 39  1 24\n",
      "  2  2 21  1 23  4 14 24  1 18  1 21 12  2  4  9  2  1 30  1 15  2  0  0  0]\n",
      "    predicted > [ 5  4  1  3  5  2  1  3  5  2  4  1 18  1  3  2  2  9  1  8  4  8  2  1  4\n",
      "  4  2  6  1  3  5  1  3  5  2  1  1  5  3  1  5  2  4  2  1  0  1  5  1  2\n",
      "  2  2  1  1  3  5  1  2  1  0  1  0  3  2  5  3  2  1  1  1  1  2  0  0  0]\n",
      "\n",
      "==> epoch 2:\n",
      "batch 0\n",
      "  minibatch loss: 1.5027602910995483\n",
      "  sample 1:\n",
      "    input     > [21 10  5 31  2 14  3  1 16 13  3  2  7 23  2 10 16 22  9  1  4 12  6 14  2\n",
      " 22  9  1  4 11 27  2  7  3 13 10  2  9  1  6  7  1 15  5  7 11  2 10 12  4\n",
      "  7 11  1 18  1 23 17  1 12  2 15  6  9  1 14  4 10 10  5 12 12]\n",
      "    predicted > [ 1  3  5  5  2  1  3  1  5  5  3  2  2  3  2  3 11  2  3  1  4  2  2  1  2\n",
      "  2  3  1  4 11  8  2  2  3  4  3  2  3  1  6  2  1  1  5  2  9  2  3  3  5\n",
      "  2 11  1  1  1  3  5  1  3  2  1  2  3  1  1  5  3  3  5  2  3]\n",
      "  sample 2:\n",
      "    input     > [ 3  8  6  9  1  2 23  5  5 24  1  6  9  1 20  5 10  1  3  8  2  1 13  9  2\n",
      "  1  5 20  1  4  7 17  5  7  2  1  4  7 17 15  8  2 10  2  1  4  3  1  7  5\n",
      "  1 14  5  9  3  1  4  7 11  1 15  6  3  8  0  0  0  0  0  0  0]\n",
      "    predicted > [ 3  5  2  4  1  2  3  5  5  2  1  2  3  1  3  5  3  1  3  5  2  1  3  3  2\n",
      "  1  5  2  1  4  2  5  5  2  2  1  4  2  5  1  5  2  3  2  1  4  3  1  2  5\n",
      "  1  1  5  3  3  1  4  2  6  1 11  2  3  5  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 4 12 19  5  9  3  1  7  5  1 10  2  9  3 10  6 14  3  6  5  7  9  1 15  8\n",
      "  4  3  9  5  2 27  2 10  1 25  1 17  5 13  1 19  4 17  1 14  5 21 17  1  6\n",
      "  3  1 18  1 16  6 27  2  1  6  3  1  4 15  4 17  1  5 10  0  0]\n",
      "    predicted > [4 4 2 5 4 3 1 2 5 1 4 2 3 3 3 2 1 3 2 5 2 3 1 1 5 4 3 3 5 2 5 2 3 1 1 1 5\n",
      " 5 3 1 2 5 5 1 1 5 1 5 1 2 3 1 0 1 5 2 8 2 1 2 3 1 5 1 5 5 1 5 3 0 0]\n",
      "\n",
      "batch 199\n",
      "  minibatch loss: 0.39362385869026184\n",
      "  sample 1:\n",
      "    input     > [26  4  7 17  1  9  8 10  6 19 21  1 14  5 13 12 11  1  8  4 27  2  1  3  5\n",
      " 12 11  1 17  5 13  1  3  8  4  3  1 25  1 22  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "    predicted > [26  4  7 17  1  9  8 10  6 19 14  1 14  5 13 12 11  1  8  4  8  2  1  3  5\n",
      " 12 11  1 17  5 13  1  3  8  4  3  1 18  1 22  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [26  6 20  1  6 22 11  1 23  2  2  7  1  3  8  2  1 15  8  6  3  6  7 16  1\n",
      " 18  1 22  1  9  4  6 11  1  4 12  6 14  2  1 18  1 15  8  5  9  2  1  3  8\n",
      "  5 13 16  8  3  9  1 15  2 10  2  1  9  3  6 12 12  1 10 13  7  7  6  7 16]\n",
      "    predicted > [26  6 10  1  6 22 11  1  4  2  2  7  1  3  8  2  1 15  8  6  3  6  7 16  1\n",
      " 18  1 22  1  9  4  6 11  1  4 12  6 14  2  1 18  1 15  8  5  9  2  1  3  8\n",
      "  5 13 16  8  3  9  1 15  2 10  2  1  9  3  6 12 12  1 10 13  7  7  6  7 16]\n",
      "  sample 3:\n",
      "    input     > [ 5  7  1  3  8  2  1  9  5  7 16  1 18  1 26  6 22 11  1  8  4 27  2  1  9\n",
      "  4  6 11  1  3  5  1  3  8  2  1 21  5 10 21  5  6  9  2  1 18  1 39  1 24\n",
      "  2  2 21  1 23  4 14 24  1 18  1 21 12  2  4  9  2  1 30  1 15  2  0  0  0]\n",
      "    predicted > [ 5  7  1  3  8  2  1  9  5  7 16  1 18  1  3  6 22 11  1  8  4  8  2  1  9\n",
      "  4  6 11  1  3  5  1  3  8  2  1 21  5 10 21  5  6  9  2  1 18  1 15  1  6\n",
      "  2  2 21  1 13  4 14 11  1 18  1  0 12  2  4  9  2  1  8  1 15  2  0  0  0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "\n",
    "try:\n",
    "    for epoch in range(2):\n",
    "        print('==> epoch {}:'.format(epoch + 1))\n",
    "        for batch in range(max_batches):\n",
    "            #print(input_seqs)\n",
    "            fd = next_feed(batch, input_seqs, output_seqs, batch_size)\n",
    "            _, l = sess.run([train_op, loss], fd)\n",
    "            loss_track.append(l)\n",
    "            #calculate validation loss\n",
    "            vl = sess.run(loss, vd)\n",
    "            vd_loss_track.append(vl)\n",
    "\n",
    "            if batch == 0 or batch % batches_in_epoch == 0:\n",
    "                print('batch {}'.format(batch))\n",
    "                print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "                predict_ = sess.run(decoder_prediction, fd)\n",
    "                for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                    print('  sample {}:'.format(i + 1))\n",
    "                    print('    input     > {}'.format(inp))\n",
    "                    print('    predicted > {}'.format(pred))\n",
    "                    if i >= 2:\n",
    "                        break\n",
    "                print()\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.3956 after 4000 examples (batch_size=10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112d9df60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track, 'b', vd_loss_track, 'r')\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test on test data\n",
    "# get the test batch\n",
    "total_data_size = len(input_seqs) # 2791\n",
    "training_data_size = max_batches * batch_size # 2000\n",
    "# test_data = total_data - training_data\n",
    "test_input_seqs  = input_seqs[training_data_size:]\n",
    "test_output_seqs = output_seqs[training_data_size:]\n",
    "\n",
    "# sort the test data on len and observe\n",
    "tiss = sorted(test_input_seqs, key=len)\n",
    "toss = sorted(test_output_seqs, key=len)\n",
    "\n",
    "# no. of batchess in test data\n",
    "test_batches = len(test_input_seqs) / batch_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 8, 4, 3]\n",
      "7\n",
      "[[ 3 24 13  3 14 24 32 19  8  3]\n",
      " [ 8  6 21  5  5  7 13  5  4  8]\n",
      " [ 4  7  1  5 13  4  2 27 11  2]\n",
      " [ 3 16 25  1 10 27  2  2  1  1]\n",
      " [ 0  1  1 25  3  2  7 11 29  2]\n",
      " [ 0 25 22  1  1  1  1  1  1  7]\n",
      " [ 0  0  0 22 25 25 25 25 22 11]]\n",
      "70\n",
      "(7, 10)\n"
     ]
    }
   ],
   "source": [
    "fd = next_feed(0, tiss, toss, batch_size)\n",
    "print(tiss[0])\n",
    "print(len(fd[encoder_inputs]))\n",
    "print(fd[encoder_inputs])\n",
    "print(fd[encoder_inputs].size)\n",
    "print(fd[encoder_inputs].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "len 7\n",
      "batch 1\n",
      "len 10\n",
      "batch 2\n",
      "len 13\n",
      "batch 3\n",
      "len 18\n",
      "batch 4\n",
      "len 22\n",
      "batch 5\n",
      "len 25\n",
      "batch 6\n",
      "len 27\n",
      "batch 7\n",
      "len 27\n",
      "batch 8\n",
      "len 28\n",
      "batch 9\n",
      "len 30\n",
      "batch 10\n",
      "len 30\n",
      "batch 11\n",
      "len 33\n",
      "batch 12\n",
      "len 35\n",
      "batch 13\n",
      "len 36\n",
      "batch 14\n",
      "len 37\n",
      "batch 15\n",
      "len 38\n",
      "batch 16\n",
      "len 41\n",
      "batch 17\n",
      "len 44\n",
      "batch 18\n",
      "len 45\n",
      "batch 19\n",
      "len 48\n",
      "batch 20\n",
      "len 51\n",
      "batch 21\n",
      "len 53\n",
      "batch 22\n",
      "len 56\n",
      "batch 23\n",
      "len 58\n",
      "batch 24\n",
      "len 60\n",
      "batch 25\n",
      "len 62\n",
      "batch 26\n",
      "len 63\n",
      "batch 27\n",
      "len 64\n",
      "batch 28\n",
      "len 64\n",
      "batch 29\n",
      "len 65\n",
      "batch 30\n",
      "len 65\n",
      "batch 31\n",
      "len 66\n",
      "batch 32\n",
      "len 66\n",
      "batch 33\n",
      "len 66\n",
      "batch 34\n",
      "len 67\n",
      "batch 35\n",
      "len 67\n",
      "batch 36\n",
      "len 67\n",
      "batch 37\n",
      "len 68\n",
      "batch 38\n",
      "len 68\n",
      "batch 39\n",
      "len 69\n",
      "batch 40\n",
      "len 69\n",
      "batch 41\n",
      "len 69\n",
      "batch 42\n",
      "len 69\n",
      "batch 43\n",
      "len 69\n",
      "batch 44\n",
      "len 70\n",
      "batch 45\n",
      "len 70\n",
      "batch 46\n",
      "len 70\n",
      "batch 47\n",
      "len 70\n",
      "batch 48\n",
      "len 71\n",
      "batch 49\n",
      "len 71\n",
      "batch 50\n",
      "len 71\n",
      "batch 51\n",
      "len 71\n",
      "batch 52\n",
      "len 71\n",
      "batch 53\n",
      "len 71\n",
      "batch 54\n",
      "len 72\n",
      "batch 55\n",
      "len 72\n",
      "batch 56\n",
      "len 72\n",
      "batch 57\n",
      "len 72\n",
      "batch 58\n",
      "len 72\n",
      "batch 59\n",
      "len 72\n",
      "batch 60\n",
      "len 73\n",
      "batch 61\n",
      "len 73\n",
      "batch 62\n",
      "len 73\n",
      "batch 63\n",
      "len 73\n",
      "batch 64\n",
      "len 73\n",
      "batch 65\n",
      "len 73\n",
      "batch 66\n",
      "len 74\n",
      "batch 67\n",
      "len 74\n",
      "batch 68\n",
      "len 74\n",
      "batch 69\n",
      "len 74\n",
      "batch 70\n",
      "len 75\n",
      "batch 71\n",
      "len 75\n",
      "batch 72\n",
      "len 75\n",
      "batch 73\n",
      "len 75\n",
      "batch 74\n",
      "len 76\n",
      "batch 75\n",
      "len 76\n",
      "batch 76\n",
      "len 77\n",
      "batch 77\n",
      "len 77\n",
      "batch 78\n",
      "len 81\n"
     ]
    }
   ],
   "source": [
    "test_loss_track_x = []\n",
    "test_loss_track_y = []\n",
    "\n",
    "#fd = next_feed(max_batches, input_seqs, output_seqs, batch_size)\n",
    "for batch in range(int(test_batches)):\n",
    "    fd = next_feed(batch, tiss, toss, batch_size)\n",
    "    print('batch {}'.format(batch))\n",
    "    ll = len(fd[encoder_inputs])\n",
    "    print('len {}'.format(ll))\n",
    "    #print(len(fd[encoder_inputs]))\n",
    "    #if(batch == 62):\n",
    "    #    print(fd)\n",
    "    l = sess.run(loss, fd)\n",
    "    #print('  minibatch loss: {}'.format(l))\n",
    "    test_loss_track_x.append(ll)\n",
    "    test_loss_track_y.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 10, 13, 18, 22, 25, 27, 27, 28, 30]\n",
      "[1.7915381, 1.4627589, 1.1829064, 0.94134808, 0.82094365, 0.66022992, 0.59418058, 0.75068945, 0.79948437, 0.85637391]\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "print(test_loss_track_x[0:10])\n",
    "print(test_loss_track_y[0:10])\n",
    "print(len(test_loss_track_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1135c2a58>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nm5kl6P8BEaj7TADCdwEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113d5a1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(test_loss_track_x, test_loss_track_y, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1137e71d0>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nm5kl6P8BEaj7TADCdwEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113677a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(test_loss_track_x, test_loss_track_y, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Different Decoder\n",
    "Defining a new decoder using raw_rnn in which input to next timestep if output of previous timestep unlike earlier when the decoder input for each timestep was given.\n",
    "\n",
    "###### following catch or issue\n",
    "* how to define length of decoder output / length of time steps / how far to run the decoder\n",
    "    * stop after fix no. of run\n",
    "    * let the network stop on its own?? - how??\n",
    "* inital hidden state is same as earlier decoder rnn, which is: encoder's final state\n",
    "* can we speedup training by providing decoder input in this layer or mix its with decoder output??\n",
    "\n",
    "Ref: [Advanced dynamic seq2seq with TensorFlow](https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/2-seq2seq-advanced.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set decoder cell\n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "#get maximum time step from input data use that to set the decoder output timestep\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for now define it a fixed length based on encoder input length e.g. +3\n",
    "decoder_lengths = encoder_inputs_length + 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define wieght and bias for the hidden layer  to output layer which map it to vocab \n",
    "# unlike earlier we used the existing linear layer\n",
    "# this will be clear later why we did this but for now it is needed as we will be giving input to next decoder time-step\n",
    "# and we have to pass this W and b in each run and earlier it was used internally and we never needed it\n",
    "# but now we are defining the decoder rnn from scratch we also need to do this\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decoder will work on single vector of column of size: batch_size in a single time step\n",
    "# EOS = end of sentence tag: defined but not used earlier - need to check how it can be used\n",
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "# defining for particular timestep of the current batch_size\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "# embedding of the above\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transition function\n",
    "Now we are going to use tf.nn.raw_rnn to create our decoder rnn so that we are able to pass output(generated token) of previous timestep to current timestep. For the we need to define loop transition function.\n",
    " * initial loop trasition function i.e. for 0th timestep \n",
    " * other timestep loop transition function\n",
    " \n",
    "Loop transition function is a mapping:  \n",
    "(time, previous_cell_output, previous_cell_state, previous_loop_state) -> (elements_finished, input, cell_state, output, loop_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial loop transition function\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = eos_step_embedded # in current scenario it will map to vocab entry with key 1\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop function for transition\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "    # given output (hidden state form) from time t transform into input for t+1 cell\n",
    "    ##  output(t) -> output projection(t) -> prediction(t) (argmax) -> input embedding(t+1) -> input(t+1)\n",
    "    def get_next_input() :\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        output = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return output\n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    next_input = tf.cond(finished, lambda: pad_step_embedded, get_next_input) # if it is already finished\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            next_input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the above 2 loop functions and define loop fn for rnn \n",
    "\n",
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "# and create the rnn using above loop fn\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "#\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temporary flatten to do logits calculation and prediction\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the entropy function\n",
    "# i.e. decoder_prediction diff decoder_target energy calculation\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
